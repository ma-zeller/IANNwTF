{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ma-zeller/IANNwTF/blob/main/IANNwTF_HW_10_Group_15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZtSksa0QgRa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f55be9d-2e98-4b0a-f474-bbb2892031c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 KB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        " !pip install -q tensorflow-text==2.9.0\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds   \n",
        "from copy import deepcopy\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "import math\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "import regex as re\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount (\"/content/drive\")\n",
        "os.chdir (\"drive/MyDrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGAa3NhjQk7P",
        "outputId": "4a1af573-9c55-49e5-acbb-131e39918f0e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import and open file\n",
        "corpus = open(\"/content/drive/MyDrive/bible.txt\")\n",
        "\n",
        "# convert object to string\n",
        "corpus = corpus.read()\n",
        "# print(corpus[0:250])"
      ],
      "metadata": {
        "id": "iY71AMyKR8Uj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "context_window = 4\n",
        "vocab_size = 10000\n",
        "batch_size = 64\n",
        "embed_size = 64\n",
        "learning_rate = 0.01\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "qFKYQSzzrZQY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove special chars\n",
        "corpus = re.sub('[^A-Za-z\\s.]+', '', corpus)\n",
        "\n",
        "# remove newline\n",
        "corpus = corpus.replace('\\n', ' ').replace('\\r', '')\n",
        "\n",
        "# convert to lowercase\n",
        "corpus = corpus.lower()\n",
        "\n",
        "# #print result\n",
        "# print(corpus[0:244])"
      ],
      "metadata": {
        "id": "o9ixvhVISBz7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize\n",
        "corpus = tf_text.WhitespaceTokenizer().split(corpus)\n",
        "print(f\"Length: {len(corpus)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwxo_IZ_XPin",
        "outputId": "263fcff9-c61c-4b62-cf50-e6656aa67b24"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length: 790017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# keep top k most common words\n",
        "y, idx, count = tf.unique_with_counts(corpus)\n",
        "filter = y[0:vocab_size]\n",
        "corpus = corpus.numpy().tolist()\n",
        "filter = filter.numpy().tolist()"
      ],
      "metadata": {
        "id": "Op_ZrTSUearx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove uncommen words from dataset (disgustingly inefficient, should use np mask and where)\n",
        "for i, x in enumerate(tqdm(corpus)):\n",
        "  if x in filter:\n",
        "    continue\n",
        "  else:\n",
        "    corpus.remove(x)\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIs_CGXnh9t_",
        "outputId": "94b05bca-8ab1-40c2-e1c1-3db40293b54c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 770324/790017 [03:05<00:04, 4156.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# back to list and reformat\n",
        "corpus = \" \".join(str(x).replace(\"b'\", \"\").replace(\"'\",\"\") for x in corpus)"
      ],
      "metadata": {
        "id": "MQvVWUJzkAJq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize again\n",
        "corpus = tf_text.WhitespaceTokenizer().split(corpus)"
      ],
      "metadata": {
        "id": "ZLmnVSKfjh1S"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"New length: {len(corpus)}\")\n",
        "for i, word in enumerate(corpus):\n",
        "  print(word)\n",
        "\n",
        "  if i == 10:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIBZu38clpVY",
        "outputId": "b59ac1bb-63ea-4a9d-b281-b83e4991a7a5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New length: 770324\n",
            "tf.Tensor(b'the', shape=(), dtype=string)\n",
            "tf.Tensor(b'first', shape=(), dtype=string)\n",
            "tf.Tensor(b'book', shape=(), dtype=string)\n",
            "tf.Tensor(b'of', shape=(), dtype=string)\n",
            "tf.Tensor(b'moses', shape=(), dtype=string)\n",
            "tf.Tensor(b'called', shape=(), dtype=string)\n",
            "tf.Tensor(b'genesis', shape=(), dtype=string)\n",
            "tf.Tensor(b'in', shape=(), dtype=string)\n",
            "tf.Tensor(b'the', shape=(), dtype=string)\n",
            "tf.Tensor(b'beginning', shape=(), dtype=string)\n",
            "tf.Tensor(b'god', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get targets by creating sequences from corpus\n",
        "targets = list()\n",
        "for i, elem in tqdm(enumerate(corpus)):\n",
        "  \n",
        "  if i == (len(corpus)-1-context_window):\n",
        "    break\n",
        "\n",
        "  else:\n",
        "    targets.append(corpus[i+1:i+1+context_window])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h57cvrZzr1N4",
        "outputId": "33a57aec-75b6-4077-f535-d1df2378edc7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "770319it [04:39, 2752.07it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length before cutoff: {len(corpus)}\")\n",
        "print(f\"Target length: {len(targets)}\")\n",
        "\n",
        "# adjust length\n",
        "corpus = corpus[:len(corpus)-(len(corpus)-len(targets))]\n",
        "print(f\"Length after cutoff: {len(corpus)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXqVeufRwKMK",
        "outputId": "2e2ee93d-66f8-41d1-aaff-90265b95e0fa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length before cutoff: 770324\n",
            "Target length: 770319\n",
            "Length after cutoff: 770319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_ds = tf.data.Dataset.from_tensor_slices(corpus)\n",
        "target_ds = tf.data.Dataset.from_tensor_slices(targets)"
      ],
      "metadata": {
        "id": "s5QFHzMwvHna"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = tf.data.Dataset.zip((corpus_ds, target_ds))"
      ],
      "metadata": {
        "id": "rJnRAI38vXvi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ds_preprocess(bible):\n",
        "\n",
        "    # batch\n",
        "    bible = bible.batch(batch_size)\n",
        "\n",
        "    # shuffle\n",
        "    bible = bible.shuffle(10000)\n",
        "\n",
        "    # prefetch\n",
        "    bible = bible.prefetch(tf.data.AUTOTUNE)\n",
        "    \n",
        "    return bible"
      ],
      "metadata": {
        "id": "JTPhZKFkrBDG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bible_ds = ds.apply(ds_preprocess)\n",
        "\n",
        "print(f\"Number training batches: {len(bible_ds)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08MDgvTwvsRd",
        "outputId": "494b1457-1997-464f-e227-32c78f986d66"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number training batches: 12037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text, target in bible_ds:\n",
        "  print(f\"Text: {text[0]}\")\n",
        "  print(f\"Target: {target[0]}\")\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPh6jHDtw6E7",
        "outputId": "89d92d52-c724-43cf-c747-b7beb2c13c51"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: b'up.'\n",
            "Target: [b'the' b'princes' b'refrained' b'talking']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/svaderia/SkipGram/blob/master/skipgram.py"
      ],
      "metadata": {
        "id": "MVjPK2PR42HG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGramModel(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "        self.lr = learning_rate\n",
        "        self.optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "        \n",
        "  \n",
        "    def build(self):\n",
        "        self.embedding_matrix = self.add_weight(shape=(self.vocab_size, self.embed_size),initializer=\"uniform\",trainable=True)\n",
        "        self.score_matrix = self.add_weight(shape=(self.vocab_size, self.embed_size),initializer=\"uniform\",trainable=True)\n",
        "\n",
        "    def call(self,inputs):\n",
        "        embed = tf.nn.embedding_lookup(self.embedding_matrix, np.asarray(self.score_matrix,dtype=np.int32))\n",
        "\n",
        "        nce_weight = tf.Variable(tf.truncated_normal([self.vocab_size, self.embed_size],\n",
        "                                                        stddev=1.0 / (self.embed_size ** 0.5)), \n",
        "                                                        name='nce_weight')\n",
        "        \n",
        "        nce_bias = tf.Variable(tf.zeros([self.vocab_size]), name='nce_bias') \n",
        "        self.loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight, \n",
        "                                                        biases=nce_bias, \n",
        "                                                        labels=inputs, \n",
        "                                                        inputs=embed, \n",
        "                                                        num_sampled=self.num_sampled, \n",
        "                                                        num_classes=self.vocab_size), name='loss')  \n",
        "\n",
        "    def train_step(self,data):\n",
        "      with tf.GradientTape() as tape:\n",
        "\n",
        "            # define a pair of a word and a target\n",
        "            input = data[0]\n",
        "            context = data[1]\n",
        "\n",
        "            input_embedding = self.call(input)\n",
        "\n",
        "            context_embedding = self.call(context)\n",
        " \n",
        "            loss = self.loss_function(context_embedding,input_embedding)\n",
        "\n",
        "      gradients = tape.gradient(loss, self.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "      return loss"
      ],
      "metadata": {
        "id": "Wwv-6AaWSxqo"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate model\n",
        "model = SkipGramModel(vocab_size, embed_size)"
      ],
      "metadata": {
        "id": "oX2n93BXrWjN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summary\n",
        "model.build()"
      ],
      "metadata": {
        "id": "440eaz4yVijp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show result\n",
        "\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "columns = 4\n",
        "rows = 2\n",
        "\n",
        "# initiate a loss list\n",
        "loss_lst = []\n",
        "\n",
        "def train(model, data):\n",
        "    # train the embedding model\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch: {epoch}\")\n",
        "\n",
        "        for pair in tqdm(data):\n",
        "            # pair is one two-word list from the sequences list of lists (the input/context pairs)\n",
        "\n",
        "            loss = model.train_step(pair)\n",
        "            loss_lst.append(loss)\n",
        "\n",
        "        # after training, the weight matrix of the model's embedding layer is the embedding space lookup_table\n",
        "        # access this so yu can work with the ultimate embedding space\n",
        "        embedding_matrix = model.embedding.weights[0]\n",
        "\n",
        "    return loss_lst, embedding_matrix\n",
        "\n",
        "def test_embeddings(test_index, embedding_list, word_index):\n",
        "\n",
        "    # embedding of text index\n",
        "    test_embed = embedding_list[test_index]\n",
        "\n",
        "    # A list for the cosine similarities\n",
        "    cosine_sim = []\n",
        "\n",
        "    # for each embedding in the embedding space\n",
        "    for embed in embedding_list:\n",
        "\n",
        "        # exclude the embedding for the test word itself\n",
        "        if np.array_equal(embed,test_embed) != True:\n",
        "\n",
        "            # compute the cosine similarity between the test word embedding and the current comparison embedding\n",
        "            cosine = np.dot(test_embed, embed) / (norm(test_embed) * norm(embed))\n",
        "\n",
        "            # add that cosine similarity value to the above-created list\n",
        "            cosine_sim.append(cosine)\n",
        "\n",
        "def loss_plot(loss_lst):\n",
        "    fig = plt.figure()\n",
        "    line1, = plt.plot(loss_list)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend([line1], [\"Loss\"])\n",
        "    plt.title(f\"Loss with {epochs} epochs\")\n",
        "    plt.show()\n",
        "\n",
        "# train the model and acquire the embeddings and the loss list for plotting\n",
        "loss_list, embeddings = train(model, bible_ds)\n",
        "\n",
        "# plot the loss\n",
        "loss_plot(loss_list)"
      ],
      "metadata": {
        "id": "qF8ESnRgdOl0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "786a075a-602d-4cd6-f354-72e37175d7ae"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/12037 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-c70f733a2da8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# train the model and acquire the embeddings and the loss list for plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mloss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbible_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# plot the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-c70f733a2da8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# pair is one two-word list from the sequences list of lists (the input/context pairs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mloss_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-f47c2ccd2a83>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0minput_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mcontext_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-f47c2ccd2a83>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         nce_weight = tf.Variable(tf.truncated_normal([self.vocab_size, self.embed_size],\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7162\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7163\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7164\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Value for attr 'Tindices' of float is not in the list of allowed values: int32, int64\n\t; NodeDef: {{node ResourceGather}}; Op<name=ResourceGather; signature=resource:resource, indices:Tindices -> output:dtype; attr=batch_dims:int,default=0; attr=validate_indices:bool,default=true; attr=dtype:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; is_stateful=true> [Op:ResourceGather]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_NAME = \"IANN_wTF_HW_10\"\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "logging_callback = tf.keras.callbacks.TensorBoard(log_dir=f\"./logs/{EXPERIMENT_NAME}/{current_time}\")"
      ],
      "metadata": {
        "id": "L_AWmRvIgtGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!kill 11736\n",
        "%tensorboard --logdir logs/"
      ],
      "metadata": {
        "id": "VZyk73PCsK-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##To DO:\n",
        "\n",
        "Kann ich die zweite variante auch benutzen?\n",
        "Transfer learning?\n",
        "Loss function"
      ],
      "metadata": {
        "id": "bUCuYcF_ktYY"
      }
    }
  ]
}