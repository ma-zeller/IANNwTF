{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95188716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ff7863",
   "metadata": {},
   "source": [
    "# Task 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb9e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(100, size=(100))\n",
    "x = (x - np.min(x))/np.ptp(x)\n",
    "x = x.astype(np.float32)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960ade2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(x_):\n",
    "    return x_**3-x_**2\n",
    "    \n",
    "myfunc_vec = np.vectorize(function)\n",
    "t = myfunc_vec(x)\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c92a804",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(10,5))\n",
    "plt.tight_layout()\n",
    "axes[0].plot(np.linspace(0,1,100))\n",
    "axes[1].plot(myfunc_vec(np.arange(0,100)))\n",
    "axes[2].plot(x)\n",
    "axes[3].plot(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9512012c",
   "metadata": {},
   "source": [
    "# Task 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec743d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_units, n_units):\n",
    "        self.weights = np.random.randn(input_units, n_units)\n",
    "        self.bias    = np.zeros((1, n_units))\n",
    "\n",
    "    def forward_step(self, x):\n",
    "    \n",
    "        self.input = x\n",
    "        \n",
    "        # save calculated outout\n",
    "        self.preactivation = self.input @ self.weights + self.bias # output\n",
    "        \n",
    "        # apply relu\n",
    "        self.activation = np.maximum(self.preactivation, 0)\n",
    "        \n",
    "        return self.activation\n",
    "\n",
    "    def backward_step(self,in_gradient):\n",
    "        \n",
    "        # print shapes\n",
    "#         print(f\"Weights: {self.input.T.shape} @ ({self.preactivation.shape} * {in_gradient.T.shape}\")\n",
    "#         print(f\"Bias: {self.preactivation.shape} * {in_gradient.T.shape}\")\"\n",
    "#         print(f\"Input_grads: {self.preactivation.shape} * {in_gradient.T.shape} @ {self.weights.T.shape}\")\n",
    "        \n",
    "        #ReLU_deriv = 1 * (self.preactivation > 0) # Why doesnt this work\n",
    "        ReLU_deriv = self.preactivation\n",
    "    \n",
    "        # get gradient wrt. weights\n",
    "        weight_grads = self.input.T @ (ReLU_deriv * in_gradient)\n",
    "        \n",
    "        # get gradient wrt. biases\n",
    "        bias_grads = (ReLU_deriv * in_gradient)\n",
    "        \n",
    "        # get gradient wrt. input\n",
    "        input_grads = (ReLU_deriv * in_gradient) @ self.weights.T   \n",
    "\n",
    "        # update step\n",
    "        self.weights = self.weights - 0.2 * weight_grads\n",
    "        self.bias = self.bias - 0.2 * bias_grads\n",
    "        \n",
    "        return input_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc3d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, components) -> None:\n",
    "        self.components = components\n",
    "\n",
    "    def forward_step(self, x):\n",
    "    \n",
    "        out = self.components[1].forward_step(x)\n",
    "        out_final = self.components[0].forward_step(out)\n",
    "        \n",
    "        return out_final\n",
    "        \n",
    "    def backward(self):\n",
    "        \n",
    "        final_layer_grad = (self.y_pred-self.y_true)\n",
    "        #print(f\"MSE Grad: {final_layer_grad}\")\n",
    "        layer2_grad = self.components[0].backward_step(final_layer_grad)\n",
    "        #print(f\"Layer2 Grad: {layer2_grad}\")\n",
    "        layer1_grad = self.components[1].backward_step(layer2_grad)\n",
    "    \n",
    "    def loss(self,y_true,y_pred):\n",
    "        \n",
    "        self.y_true = y_true\n",
    "        self.y_pred = y_pred\n",
    "        \n",
    "        return 0.5*(y_pred-y_true)**2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c3b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = Layer(1,10)\n",
    "layer2 = Layer(10,1)\n",
    "\n",
    "m = Model([layer2, layer1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f6c721",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "\n",
    "for epoch in range (0,1000):\n",
    "    \n",
    "    running_loss = 0\n",
    "    \n",
    "    for idx, datapoint in enumerate(x):        \n",
    "        \n",
    "        #print(f\"sample: {np.broadcast_to(datapoint,(1,))}\")\n",
    "        #print(f\"Real: {np.broadcast_to(t[idx],(1,))}\")\n",
    "        \n",
    "        # fwp\n",
    "        out = m.forward_step(np.broadcast_to(datapoint,(1,)))\n",
    "        \n",
    "        # update loss\n",
    "        running_loss += m.loss(t[idx],out)\n",
    "        \n",
    "        # bwp\n",
    "        m.backward()\n",
    "         \n",
    "    loss.append((running_loss/len(x))[0])\n",
    "    print(f\"Loss in Epoch {epoch+1}: {(running_loss/len(x))[0]}\")\n",
    "    #break\n",
    "    \n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6bdc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)\n",
    "plt.title(\"Average Loss per Epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c28b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10,5))\n",
    "plt.tight_layout()\n",
    "\n",
    "plot = []\n",
    "\n",
    "for point in np.linspace(0,1,100):\n",
    "    out = m.forward_step(np.broadcast_to(point,(1,)))\n",
    "    plot.append(out[0])\n",
    "    \n",
    "axes[0].plot(np.linspace(0,1,100))\n",
    "axes[0].set_title(\"Input\")\n",
    "axes[0].set_ylim(0,1)\n",
    "axes[1].plot(plot)\n",
    "axes[1].set_title(\"Prediction\")\n",
    "axes[1].set_ylim(0,1000000)\n",
    "axes[2].plot(myfunc_vec(np.arange(0,100)))\n",
    "axes[2].set_title(\"True Output\")\n",
    "axes[2].set_ylim(0,1000000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (iannwtf)",
   "language": "python",
   "name": "iannwtf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
